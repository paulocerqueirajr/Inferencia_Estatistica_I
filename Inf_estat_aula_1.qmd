---
format:
  revealjs:
    theme: ["theme/q-theme.scss"]
    slide-number: c/t
    #logo: "faest.png"
    #footer: "[https://github.com/paulocerqueirajr](https://https://github.com/paulocerqueirajr)"
    code-copy: true
    center-title-slide: false
    lang: pt
    transition: fade
    transition-speed: default
highlight-style: a11y
code-link: true
height: 1080
width: 1600
execute: 
  eval: true
  echo: true
---

<h1> Inferência Estatística I </h1>

<h2> Amostra aleatória </h2>

<hr>

<br>

<h3> Prof. Paulo Cerqueira Jr <br>
Faculdade de Estatística - FAEST <br> 
Instituto de Ciências Exatas e Naturais - ICEN
</h3>

<h3>  </h3>
<br>

<h3> [https://github.com/paulocerqueirajr](https://https://github.com/paulocerqueirajr)

![](github.jpg){.absolute top=560 left=845 height="80"}



![](faest.png){.absolute top=5 left=1400 height="200"}

<!-- ![](https://www.faest.icen.ufpa.br/images/110.png){.absolute top=5 left=1400 height="200"} -->


# [Introdução]{style="float:right;text-align:right;"} {background-color="#027eb6"}

## Introdução


* Os avanços científicos são, na maioria das vezes, atribuídos aos experimentos realizados.

* Um pesquisador realiza o experimento e obtém dados.

* Baseado nos dados, algumas conclusões podem ser retiradas. 

* Estas conclusões vão, geralmente, além dos que foi observado nos dados.

* Dessa forma, o pesquisador generaliza, partindo de um experimentos para os demais que são similares. 

* Esta generalização é denominada de `Inferência`.


:::{.callout-important}
## Uma função da Estatística: 

Fornecer um conjunto de metodologias para realizar a inferência e medir o grau de incerteza dessa inferência, através da `teoria das probabilidades`.
:::

## Introdução
  
::: {#exm-ex1}
-   Suponha um recepiente com 10 milhões de sementes de flores.

-   Cada semente pode produzir flores brancas ou vermelhas.

-   Pergunta-se: Qual a porcentagem de flores brancas que serão geradas?

-   Para saber o resultado real, teríamos que plantar todas as sementes.

-   Seria uma tarefa muito trabalhosa!

-   Solução: Plantar algumas sementes, e baseando-se nos resultados podemos obter alguma informação para a porcentagem de flores brancas.
:::


## População e amostra


::: {#def-def1}
## População
É um conjunto que contém todos os elementos do problema a ser discutido, com pelo menos uma característica em comum. Desejamos obter informação sobre esta população.
:::

::: {#exm-ex2}

-   Preços da carne em um mês na região metropolitana de Belém.

-   Preços do pão em certo dia em Belém.

-   Produção de leite por animal em uma fazenda.

-   Queremos estudar a proporção de votos para um determinado candidato ao governo do Estado do Pará.

-   Queremos estudar o grau de satisfação dos usuários de uma determinada operadora de telefonia celular.

:::


# [População e amostra]{style="float:right;text-align:right;"} {background-color="#027eb6"}


## População e amostra


:::{#def-def2}
## Amostra aleatória - a.a
Sejam $X_1, X_2, \dots, X_n$ uma sequência de variáveis aleatórias com distribuição conjunta $f_{X_1, \dots, X_n}(x_1, \dots, x_n)$ que fatora como na seguinte igualdade:

$$f_{X_1, \dots, X_n}(x_1, \dots, x_n)= f_{X_{1}}(x_{1})\times f_{X_{2}}(x_{2})\times \dots \times f_{X_{n}}(x_{n})=\prod\limits_{i=1}^{n}f_{X_{i}}(x_{i}),$$

em que $f(\cdot)$ é a função de probabilidade (f.p) ou função de densidade de probabilidade (f.d) para cada $X_i$ . Então, $X_1, X_2, \dots, X_n$ é uma amostra aleatória de tamanho $n$ retirada de uma população com p.d/f.d.
:::

## População e amostra


:::{#exm-ex3}
Imagine 10 milhões de sementes em um recepiente e a produção de flores brancas e vermelhas.


* `População:` Sementes dentro do recipiente.

* `Unidade experimental:` Uma semente.

* `Característica:` Flor branca ou vermelha.

::: 


-   Não temos um valor numérico associado a cada elemento, mas podemos definir o seguinte tipo de resposta:

$$\text{Flor branca} =1\  \text{e} \ \text{Flor vermelha} =0.$$

`Variável aleatória:` $X_{i}=1$ ou $X_{i}=0$, para $i=1,2, \dots, n$.

## Poupulação e amostra


* A variável aleatória $X_i$ é uma representação do valor numérico que a $i-$ésima unidade amostral irá assumir.

* Depois que a amostra $X_1,X_2,\dots, X_n$ observada os valores serão conhecidos e denotados por $x_1,x_2,\dots, x_n$.


* Logo:

Suponha que $X$ pode assumir apenas os valores 0 ou 1 com
probabilidades $1-\theta$ e $\theta$, respectivamente. Então, sejam $X_1, X_2, \dots, X_n$ uma a.a de $X\sim Ber(\theta)$, sua distribuição conjunta $P(X_{1}=x_{1};X_{2}=x_{2};\dots; X_{n}=x_{n})$ é dada por

$$=\theta^{x_{1}}(1-\theta)^{1-x_{1}}\times \theta^{x_{2}}(1-\theta)^{1-x_{2}}\times \dots \times \theta^{x_{n}}(1-\theta)^{1-x_{n}}$$ $$=\theta^{x_{1}+x_{2}+\dots+x_{n}}(1-\theta)^{(1-x_{1})+(1-x_{2})+\dots+(1-x_{n})}$$ $$=\theta^{\sum_{i=1}^{n}x_{i}}(1-\theta)^{\sum_{i=1}^{n}(1-x_{i})}$$ $$=\theta^{\sum_{i=1}^{n}x_{i}}(1-\theta)^{(n-\sum_{i=1}^{n}x_{i})}$$



# [Estatísticas e Parâmetros]{style="float:right;text-align:right;"} {background-color="#027eb6"}

## Estatísticas e Parâmetros - Introdução

:::{.callout-note}
## Um dos problemas principais da Estatística envolve o seguinte:

* Estudar uma população com f.p/f.d $f (\cdot\mid \theta)$, onde a forma da f.p/f.d é conhecida com parêmetro desconhecido $\theta$. 

* Se $\theta$ fosse conhecido f.p/f.d estaria completamente especificada.
:::

:::{.callout-note}
## Procedimento de inferência envolverá:

* A obtenção de uma amostra aleatória  $X_1, X_2, \dots, X_n$ desta f.p/f.d.

* O uso de uma função $T(x_1, x_2, \dots, x_n)$ como estimativa para o parâmetro $\theta$ (desconhecido).
:::


## Estatísticas

* O problema aqui consiste em determinar qual será a melhor função para estimar $\theta$.

* Iremos avaliar certas funções (funções amostrais) de uma amostra aleatória.

:::{#def-def3}
## Estatísticas

É uma função da amostra, $T(x) = f (X_1, X_2, \dots, X_n)$, representando uma característica da amostra.
:::

. . .

:::{.callout-important}
## Importante:

A formulação de uma estatística `não pode envolver quantidades desconhecidas`.
:::



## Estatísticas

> Os exemplos mais comuns:

-   Média amostral: $\overline{X}=\dfrac{1}{n}\sum\limits_{i=1}^{n}X_{i}.$

-   Variância amostral: $\hat{\sigma^{2}}=\dfrac{1}{n}\sum\limits_{i=1}^{n}(X_{i}-\overline{X})^{2}.$

-   Mediana amostral: $\tilde{X}=\text{med}(X_1, X_2, \dots, X_n).$

-   Mínimo amostral: $X_{(1)}=\min(X_1, X_2, \dots, X_n).$

-   Máximo amostral: $X_{(n)}=\max(X_1, X_2, \dots, X_n).$

-   Ponto médio amostral: $\dfrac{1}{2}(X_{(1)}+X_{(n)}).$


## Parâmetros

:::{#def-def4}
## Parâmetro

Uma parâmetro é uma medida (desconhecida) usada para descrever uma característica da população.
:::

. . .

* As relação das estatísticas com seus respectivos parâmetros:


| Medida           |    Estatística     | Parâmetro  |
|------------------|:------------------:|:----------:|
| Média            |   $\overline{X}$   |   $\mu$    |
| Variância        | $\hat{\sigma^{2}}$ | $\sigma^2$ |
| $N$ de elementos |        $n$         |    $N$     |
| Proporção        |   $\hat{\theta}$   |  $\theta$  |


## Estatísticas e Parâmetros

:::{#exm-exm4}
Considere uma variável aleatória observável com f.d:
:::
-   $f(x)=N\left[ x \mid \mu, \sigma^2 \right]$, com $\mu$ e $\sigma$ desconhecidos.

-   Logo,

$$X-\mu \ \text{e}\ X/\sigma\ \text{são Estatísticas??}$$

. . . 

-   `Não são`, pois contém elementos desconhecidos.

. . . 

-   $X$, $X+3$ e $X^2+\log X^2$ `são estatísticas`.


## Estatísticas e Parâmetros

:::{#exm-exm5}
Seja $X_1, X_2, \dots, X_n$ uma amostra aleatória com f.p/f.d $f(\cdot;\theta)$ então:

$$\overline{X}_{n}=\dfrac{1}{n}\sum_{i=1}^{n}X_{i} \quad \text{e} \quad\dfrac{1}{2}\left\{\min(X_1,\dots, X_n)+\max(X_1, \dots, X_n)\right\}$$ 

`são exemplos` de estatísticas.

:::

. . .

:::{#exm-exm6}
Se $f(x;\theta)=N\left[ x \mid \theta, 1 \right]$, com $\theta$ desconhecido.
:::

$$\overline{X}_{n}-\theta \quad \text{é uma Estatística?}$$ 

. . .

* `Não` é uma estatística, pois depende de $\theta$.



# [Momentos amostrais]{style="float:right;text-align:right;"} {background-color="#027eb6"}

## Momentos amostrais

Seja $X_1, \dots, X_n$ uma a.a com f.p/f.d $f(\cdot)$. O r-ésimo momento amostral em relação à $0$ é definido por

$$M_{r}^{'}=\dfrac{1}{n}\sum\limits_{i=1}^{n}\left(X_{i}-0\right)^r=\dfrac{1}{n}\sum\limits_{i=1}^{n}\left(X_{i}\right)^r.$$

-   Em particular, quando $r=1$, temos a média amostral $\overline{X}$ ou $\overline{X}_{n}$, em que

$$\overline{X}_{n}=\dfrac{1}{n}\sum\limits_{i=1}^{n}\left(X_{i}\right).$$

O r-ésimo momento em relação à $\overline{X}_{n}$ é dado por

$$M_{r}=\dfrac{1}{n}\sum\limits_{i=1}^{n}\left(X_{i}-\overline{X}_{n}\right)^r$$


## Momentos amostrais

> Momentos amostrais são exemplos de estatísticas!

:::{#thm-thm1} 
## Momentos Amostrais

Seja $X_1, \dots, X_n$ uma a.a com f.p/f.d $f(\cdot)$. O valor esperado do r-ésimo momento amostral (em relação à $0$) é igual ao r-ésimo momento populacional, isto é,

$$E(M_{r}^{'})=\mu_{r}^{'}, \ \text{se} \ \mu_{r}^{'}\  \text{existir}.$$

:::
-   Temos que $\mu_{r}^{'}=E(X^{r})$ é o r-ésimo momento populacional de uma população com f.p/f.d $f(x)=f_{X}(x)$.

-   Além disso:

$$Var(M_{r}^{'})=\dfrac{1}{n}\left[ E(X^{2r})-E^{2}(X^{r}) \right]$$

$$=\dfrac{1}{n}\left[ \mu_{2r}^{'}-(\mu_{r}^{'})^{2} \right].$$


## Momentos amostrais

`Demonstração:(cont.)`

> A média:

$$E(M_{r}^{'})=E\left[\dfrac{1}{n}\sum\limits_{i=1}^{n}\left(X_{i}\right)^r\right]=\dfrac{1}{n}E\left[\sum\limits_{i=1}^{n}\left(X_{i}\right)^r\right]=$$ $$=\dfrac{1}{n}\sum\limits_{i=1}^{n}E\left[\left(X_{i}\right)^r\right]=\dfrac{1}{n}\sum\limits_{i=1}^{n}\mu_{r}^{'}=\mu_{r}^{'}.$$

> A variância:

$$Var(M_{r}^{'})=Var\left[\dfrac{1}{n}\sum\limits_{i=1}^{n}\left(X_{i}\right)^r\right]=\dfrac{1}{n^2}Var\left[\sum\limits_{i=1}^{n}\left(X_{i}\right)^r\right]$$

## Momentos amostrais

`Demonstração:(cont.)`

Supondo independência, temos

$$Var(M_{r}^{'})=\dfrac{1}{n^2}\sum\limits_{i=1}^{n}Var\left[\left(X_{i}\right)^r\right]=\dfrac{1}{n^2}\sum\limits_{i=1}^{n}\left[E\left(X_{i}\right)^{2r}-E^2\left(X_{i}^{r}\right)\right]$$

$$=\dfrac{1}{n^2}\sum\limits_{i=1}^{n}\left[\mu_{2r}^{'}-(\mu_{r}^{'})^2\right]=\dfrac{1}{n}\left[\mu_{2r}^{'}-(\mu_{r}^{'})^2\right].$$

## Momentos amostrais

Quando $r=1$, temos o seguinte corolário.

:::{#cor-cor1}
Seja $X_1, \dots, X_n$ uma a.a com f.p/f.d $f(\cdot)$ e se $\overline{X}_{n}=\dfrac{1}{n}\sum\limits_{i=1}^{n}\left(X_{i}\right)$ é a média amostral, então,

$$E(\overline{X}_{n})=\mu, \ \text{e} \ Var(\overline{X}_{n})=\dfrac{\sigma^2}{n}.$$ em que $\mu$ e $\sigma^2$ são a média e a variância de $f(\cdot)$.

:::

## Momentos amostrais

-   O @thm-thm1 fornece a média e a variância, em termos de momentos populacionais, do r-ésimo momento amostral em relação a 0.

-   Um resultado similar, porém mais complicado, pode ser derivado para a média e variância do r-ésimo momento amostral em relação a média amostral.

-   Considere $r=2$, tal que $M_{2}=\dfrac{1}{n}\sum\limits_{i}(X_{i}-\overline{X})^2.$

-   $M_2$ as vezes é chamado de `variância amostral`.

-   Entretanto, definiremos a variância amostral de outra forma.

## Momentos amostrais

:::{#def-def6}
Seja $X_1, \dots, X_n$ uma a.a com f.p/f.d $f(\cdot)$,

$$S^2_{n}=S^{2}=\dfrac{1}{n-1}\sum\limits_{i}(X_{i}-\overline{X})^2, \ \text{para} \ n>1,$$

é definida como **variância amostral**.

:::

> A razão para considerarmos $S^2$ ao invés de $M_{2}$ como variância é devido $$E(S^2)=\sigma^2 \ \text{e} \ E(M_2)\neq\sigma^2$$

-   Revisando: $\mu_{r}^{'}=E(X^r)$ é o r-ésimo momento de $X$ (em relação a 0).

## Momentos amostrais

:::{#def-def7} 
## Momento central

O r-ésimo momento central de uma variável aleatória $X$ com relação ao ponto $\textbf{a}$ é definido por

$$\mu_r=E\left[(X-\textbf{a})^r\right]$$
:::

- Se $\textbf{a}=E(X)=\mu$, temos $\mu_r=E\left[(X-\textbf{a})^r\right]$, então

$$\mu_{1}=E\left[(X-\mu)^1\right]=0 \quad \text{e} \quad \mu_{2}=E\left[(X-\mu)^2\right]=Var(X)=\sigma^2.$$

:::{#thm-thm2}
Seja $X_1, \dots, X_n$ uma a.a com f.p/f.d $f(\cdot)$ e seja $$S^{2}=\dfrac{1}{n-1}\sum\limits_{i}(X_{i}-\overline{X})^2,$$Então, $$E(S^2)=\sigma^2 \quad \text{e} \quad Var(S^2)=\dfrac{1}{n}\left[\mu_{4}-\dfrac{n-3}{n-1}\sigma^2\right].$$

:::

## Momentos amostrais

**Prova:** Para $E(S^2)=\sigma^2$, temos que $\sigma^{2}=E\left[ (X-\mu)^2\right]$ e $\mu_{r}=E\left[(X-\mu)^r\right]$. Note que,

$$
\begin{array}{lll}
\sum\limits_{i}(X_{i}-\mu)^2 & = &  \sum\limits_{i}\left(X_{i}+\overline{X}-\overline{X}-\mu\right)^2\\
&=& \sum\limits_{i}\left[\left(X_{i}-\overline{X}\right)^2-2\left(X_{i}-\overline{X}\right)\left(\overline{X}-\mu\right)+\left(\overline{X}-\mu\right)^2\right]\\
&=&\sum\limits_{i}\left(X_{i}-\overline{X}\right)^2-2\left(\overline{X}-\mu\right)\sum\limits_{i}\left(X_{i}-\overline{X}\right)+n\left(\overline{X}-\mu\right)^2\\
&=& \sum\limits_{i}\left(X_{i}-\overline{X}\right)^2-2\left(\overline{X}-\mu\right)\underbrace{\left(n\overline{X}-n\overline{X}\right)}_{0}+n\left(\overline{X}-\mu\right)^2\\
&=& \sum\limits_{i}\left(X_{i}-\overline{X}\right)^2+n\left(\overline{X}-\mu\right)^2\\
\end{array}
$$

## Momentos amostrais

**Prova (cont.):**

Assim,

$$
\begin{array}{lll}
E(S^2)&=& E\left[ \dfrac{1}{n-1}\sum\limits_{i}(X_{i}-\overline{X})^2\right]\\
&=& E\left[ \dfrac{1}{n-1}\sum\limits_{i}\left(X_{i}-\mu\right)^2-n\left(\overline{X}-\mu\right)^2\right]\\
&=& \dfrac{1}{n-1}E\left[ \sum\limits_{i}\left(X_{i}-\mu\right)^2-n\left(\overline{X}-\mu\right)^2\right]\\
&=& \dfrac{1}{n-1}E\left[ \sum\limits_{i}\left(X_{i}-\mu\right)^2\right]-\dfrac{n}{n-1}E\left[\left(\overline{X}-\mu\right)^2\right]\\
&=& \dfrac{n}{n-1}\sigma^2-\dfrac{n}{n-1}Var(\overline{X})\\
&=& \dfrac{n}{n-1}\sigma^2-\dfrac{n}{n-1}\dfrac{\sigma^2}{n}\\
&=& \sigma^2
\end{array}
$$

## Momentos amostrais

-   De forma similar,

$$
\begin{array}{lll}
E(M_2)&=& E\left[ \dfrac{1}{n}\sum\limits_{i}(X_{i}-\overline{X})^2\right]\\
&=& \dfrac{1}{n}\sigma^2-\dfrac{n}{n}Var(\overline{X})\\
&=& \sigma^2\left(\dfrac{n-1}{n}\right)
\end{array}
$$

-   Momentos amostrais são exemplos de **exemplos de estatísticas** que podem ser usados para estimar quantidades populacionais.

-   Por exemplo:

    -   $M_{r}^{'}$ para estimar $\mu_{r}^{'}$;
    -   $\overline{X}$ para estimar $\mu$;
    -   $S^2$ para estimar $\sigma^2$.



# [Função de verossimilhança]{style="float:right;text-align:right;"} {background-color="#027eb6"}


## Função de verossimilhança


:::{#def-def5} 
## Função de verossimilhança
A f.p/p.d.f conjunta é denominada função de verossimilhança de $\theta$, correspondente a amostra observada $\textbf{x} = (x_1, x_2, \dots, x_n)$ e será denotada por

$$L(\theta\mid \textbf{x})=\prod\limits_{i=1}^{n}f(x_{i}\mid\theta)=f(x_{1}\mid \theta)\times f(x_{2}\mid \theta)\times \dots \times f(x_{n}\mid \theta).$$
:::


Dada a amostra $\textbf{x} = (x_1, x_2, \dots, x_n)$, podemos encontrar o `ponto mais verossímil` para $\theta$.



## Função de verossimilhança

:::{#exm-exm7}
## Caso discreto
Sejam $X_1, X_2, \dots, X_n$ uma a.a de $X\sim Pois(\theta)$. Temos que a função de verossimilhança é dada por
:::


\begin{equation*}
    \begin{array}{ccl}
              L(\theta\mid \textbf{x})&=  & \prod\limits_{i=1}^{n}\dfrac{\exp\{-\lambda\}\lambda^{x_{i}}}{x_i!} \\
              & = &\dfrac{\exp\{-\lambda\}\lambda^{x_{1}}}{x_1!}\times \dfrac{\exp\{-\lambda\}\lambda^{x_{2}}}{x_2!}\times \dots \times \dfrac{\exp\{-\lambda\}\lambda^{x_{n}}}{x_n!}\\
              & = & \dfrac{\exp\{-n\lambda\}\lambda^{\sum_{i=1}^{n}x_{i}}}{\prod_{i=1}^{n}x_i!}.
          \end{array}
\end{equation*}



## Função de verossimilhança


:::{#exm-exm8}
## Caso contínuo

Sejam $X_1, X_2, \dots, X_n$ uma a.a de $X\sim N(\mu, \sigma^2)$. Temos que a função de verossimilhança é dada por

:::


\begin{equation*}
    \begin{array}{cl}
              = & \prod\limits_{i=1}^{n} \dfrac{1}{\sqrt{2\pi \sigma^2}}\exp\left\{ \dfrac{1}{2\sigma^2} (x_i - \mu)^2 \right\}\\
              = & \dfrac{1}{\sqrt{2\pi \sigma^2}}\exp\left\{ \dfrac{1}{2\sigma^2} (x_1 - \mu)^2 \right\}\times \dots \times \dfrac{1}{\sqrt{2\pi \sigma^2}}\exp\left\{ \dfrac{1}{2\sigma^2} (x_n - \mu)^2 \right\}\\
              = & \left(\dfrac{1}{\sqrt{2\pi \sigma^2}}\right)^n \exp\left\{ \dfrac{1}{2\sigma^2} \sum_{i=1}^{n}(x_i - \mu)^2 \right\}\\
              = & \left(\dfrac{1}{2\pi \sigma^2}\right)^{n/2} \exp\left\{ \dfrac{1}{2\sigma^2} \sum_{i=1}^{n}(x_i - \mu)^2 \right\}\\
              = & \left(2\pi \sigma^2\right)^{-n/2} \exp\left\{ \dfrac{1}{2\sigma^2} \sum_{i=1}^{n}(x_i - \mu)^2 \right\}
          \end{array}
\end{equation*}


# [Distribuição amostral]{style="float:right;text-align:right;"} {background-color="#027eb6"}


## Introdução

::: columns
::: column


`Para este caso temos:`

-   $X:$ Variável de interesse;

-   $\theta:$ parâmetro de interesse;

-   $T=f(X_1, X_2, \dots, X_n)$ Função da amostra que vai fornecer informação sobre $\theta$.
:::

::: column

![](pop_amostra.png)

$T$ é uma variável aleatória.


`Pergunta:` Qual a distribuição de $T$ quando $X_1, X_2, \dots, X_n$ assume valores observados?
:::
:::

# [Distribuição amostral da média]{style="float:right;text-align:right;"} {background-color="#027eb6"}



## Distribuição amostral da média

:::{#exm-exm9}

Suponha que selecionamos todas as amostras de tamanho 2, com reposição, da população $\{1, 3, 5, 5, 7\}$

:::
<!-- \begin{center} -->

<!--   \begin{tabular}{ccccc}\hline -->

<!--   $X$ & 1& 3& 5 & 7\\ \hline -->

<!--   $P(X=x)$ &1/5 &1/5 &2/5 &1/5\\ \hline -->

<!--   \end{tabular} -->

<!-- \end{center} -->


|   $X$    |  1  |  3  |  5  |  7  |
|:--------:|:---:|:---:|:---:|:---:|
| $P(X=x)$ | 1/5 | 1/5 | 2/5 | 1/5 |


-   Encontrar a distribuição conjunta da v.a. $(X_1, X2)$, sendo $X_1$ sendo o número selecionado na primeira extração e $X_2$ o número da segunda.

-   Encontre a distribuição de $\overline{X}=\dfrac{X_1 + X_2}{2}$.


## Distribuição amostral da média

::: columns
::: column

<!-- \begin{center} -->

<!--   \scalebox{0.7}{\begin{tabular}{ccccc}\hline -->

<!--   Combinação & Prob.& $X_1$ & $X_2$ & $\overline{X}$\\ \hline -->

<!--   $(1,1)$ & 1/25 &1 &1 &1\\ -->

<!--   $(1,3)$ & 1/25 &1 &3 &2\\ -->

<!--   $(1,5)$ & 2/25 &1 &5 &3\\ -->

<!--   $(1,7)$ & 1/25 &1 &7 &4\\ -->

<!--   $(3,1)$ & 1/25 &3 &1 &2\\ -->

<!--   $(3,3)$ & 1/25 &3 &3 &3\\ -->

<!--   $(3,5)$ & 2/25 &3 &5 &4\\ -->

<!--   $(3,7)$ & 1/25 &3 &7 &5\\ -->

<!--   $(5,1)$ & 2/25 &5 &1 &3\\ -->

<!--   $(5,3)$ & 4/25 &5 &3 &4\\ -->

<!--   $(5,5)$ & 2/25 &5 &5 &5\\ -->

<!--   $(5,7)$ & 1/25 &5 &7 &6\\ -->

<!--   $(7,1)$ & 1/25 &5 &7 &4\\ -->

<!--   $(7,3)$ & 1/25 &5 &7 &5\\ -->

<!--   $(7,5)$ & 1/25 &5 &7 &6\\ -->

<!--   $(7,7)$ & 1/25 &5 &7 &7\\\hline -->

<!--   \end{tabular}} -->

<!-- \end{center} -->

| Combinação | Prob. | $X_1$ | $X_2$ | $\overline{X}$ |
|:----------:|:-----:|:-----:|:-----:|:---------:|
|  $(1,1)$   | 1/25  |   1   |   1   |     1     |
|  $(1,3)$   | 1/25  |   1   |   3   |     2     |
|  $(1,5)$   | 2/25  |   1   |   5   |     3     |
|  $(1,7)$   | 1/25  |   1   |   7   |     4     |
|  $(3,1)$   | 1/25  |   3   |   1   |     2     |
|  $(3,3)$   | 1/25  |   3   |   3   |     3     |
|  $(3,5)$   | 2/25  |   3   |   5   |     4     |
|  $(3,7)$   | 1/25  |   3   |   7   |     5     |
|  $(5,1)$   | 2/25  |   5   |   1   |     3     |
|  $(5,3)$   | 4/25  |   5   |   3   |     4     |
|  $(5,5)$   | 2/25  |   5   |   5   |     5     |
|  $(5,7)$   | 1/25  |   5   |   7   |     6     |
|  $(7,1)$   | 1/25  |   5   |   7   |     4     |
|  $(7,3)$   | 1/25  |   5   |   7   |     5     |
|  $(7,5)$   | 1/25  |   5   |   7   |     6     |
|  $(7,7)$   | 1/25  |   5   |   7   |     7     |

:::

::: column
`Distribuição conjunta:`

<!-- \begin{center} -->

<!--   \scalebox{0.7}{\begin{tabular}{c|ccccc} \hline -->

<!--   $X_{1}/X_{2}$ & 1& 3 & 5 & 7 & Total \\ \hline -->

<!--   1 & 1/25& 1/25 & 2/25 & 1/25 & 1/5 \\ \hline -->

<!--   3 & 1/25& 1/25 & 2/25 & 1/25 & 1/5 \\ \hline -->

<!--   5 & 2/25& 2/25 & 4/25 & 2/25 & 2/5 \\ \hline -->

<!--   7 & 1/25& 1/25 & 2/25 & 1/25 & 1/5 \\ \hline -->

<!--   Total & 1/5& 1/5 & 2/5 & 1/5 & Total \\ \hline -->

<!--   \end{tabular}} -->

<!-- \end{center} -->

| $X_{1}/X_{2}$ |  1   |  3   |  5   |  7   | Total |
|:-------------:|:----:|:----:|:----:|:----:|:-----:|
|       1       | 1/25 | 1/25 | 2/25 | 1/25 |  1/5  |
|       3       | 1/25 | 1/25 | 2/25 | 1/25 |  1/5  |
|       5       | 2/25 | 2/25 | 4/25 | 2/25 |  2/5  |
|       7       | 1/25 | 1/25 | 2/25 | 1/25 |  1/5  |
|     Total     | 1/5  | 1/5  | 2/5  | 1/5  | Total |

:::
:::

## Distribuição amostral da média

<!-- ::: columns -->
<!-- ::: column -->

`Distribuição amostral da média` $\overline{X}$:

<!-- \scalebox{0.65}{ -->

<!-- \begin{tabular}{cccccccc}\hline -->

<!--   $\overline{X}$ & 1 & 2& 3 & 4 & 5 & 6 & 7\\ \hline -->

<!--   $P(\overline{X}=\overline{x})$ & 1/25 &2/25 &5/25 & 6/25 & 6/25 & 4/25 & 1/25\\ \hline -->

<!-- \end{tabular}} -->

|      $\overline{X}$       |        1        |        2        |        3        |        4        |        5        |        6        |        7        |
|:-----:|:---:|:---:|:---:|:---:|:----:|:---:|:---:|
| $P(\overline{X}=\overline{x})$ | $\dfrac{1}{25}$ | $\dfrac{2}{25}$ | $\dfrac{5}{25}$ | $\dfrac{6}{25}$ | $\dfrac{6}{25}$ | $\dfrac{4}{25}$ | $\dfrac{1}{25}$ |

<!-- ::: -->

<!-- ::: column -->
```{r fig1,warning=FALSE,comment="",message=FALSE,fig.height=4,fig.width=4,fig.align='center',}
require(ggplot2)
mx    <- c(1, 2, 3, 4, 5, 6, 7)
pmx   <-c(1/25, 2/25, 5/25, 6/25, 6/25, 4/25, 1/25)
dados <- data.frame(mx, pmx)
ggplot(data=dados, aes(x = factor(mx), ymin=0, ymax=pmx))+geom_linerange()+
  scale_x_discrete(breaks=1:7)+ylab("P(X=x)")+
  xlab("Média amostral") + theme_bw()
```
<!-- ::: -->
<!-- ::: -->


## Distribuição amostral da média

* O primeiro momento amostral é a média definida como:

$$\overline{X}=\overline{X}_{n}=\dfrac{1}{n}\sum\limits_{i=1}^{n}\left(X_{i}\right).$$


onde $X_{1}, X_{2}, \dots, X_{n}$ é uma amostra aleatória com f.p/f.d $f(\cdot)$.


* $\overline{X}$ é função das v.a $X_{1}, X_{2}, \dots, X_{n}$ e, portanto a distribuição pode ser encontrada teoricamente.

* Pode ser útil pensar na média amostral $\overline{X}$ como uma estimativa da média $\mu$ da f.p/f.d $f(\cdot)$ a partir de qual amostra foi selecionada.

> Um dos objetivos da amostragem é estimar $\mu$ a partir de $\overline{X}$.

## Distribuição amostral da média

:::{#thm-thm3}

Seja $X_1, X_2, \dots, X_n$ uma a.a com f.p/f.d $f(\cdot)$, média $\mu$ e variância $\sigma^2$. Considere:

$$\overline{X}=\dfrac{1}{n}\sum\limits_{i=1}^{n}\left(X_{i}\right).$$
Então, $E(\overline{X})=\mu_{\overline{X}}=\mu$ e $Var(\overline{X})=\sigma_{\overline{X}}^{2}=\dfrac{\sigma^2}{n}$.

$E(\overline{X})=\mu$: diz que em média $\overline{X}$ é igual ao parâmetro $\mu$ sendo estimado ou que a distribuição de $\overline{X}$ está centrada em $\mu$.

$Var(\overline{X})=\dfrac{\sigma^2}{n}$: diz que a dispersão dos valoers de $\overline{X}$ em torno de $\mu$ é pequena para amostras grandes em comparação com tamanhos menores.

:::


# [Teoremas de convergência]{style="float:right;text-align:right;"} {background-color="#027eb6"}

## Teoremas de convergência


* Para amostras grandes, os valores de $\overline{X}$ (que são usados para estimar $\mu$) tendem a estar mais concentrados de $\mu$ do que em amostras pequenas.

* Esta noção será definida pela **Lei dos Grandes Números**

* Seja $E(X) = \mu$ para a f.p/f.d $f(\cdot)$. Desejamos estimar $\mu$.

* De maneira não rigorosa, $E(X)$ é a média de um número infinito de valores da variável aleatória $X$.

* Em qualquer problema real podemos observar apenas um número finito de valores da variável aleatória $X$.

* Questão: Usando apenas um número finito de valores de $X$ (uma amostra aleatória de tamanho $n$) pode ser feita qualquer inferência confiável sobre $E(X)$? **A resposta é sim**.

* Usaremos isso através da Lei Fraca dos
Grandes Números.


## Teoremas de convergência


:::{#thm-thm4} 
## Lei fraca dos Grandes Números

Seja $X_1, X_2, \dots, X_n$ uma a.a de tamanho $n$ de uma população com variável $X$, com média $E(X)=\mu$ e $Var(X)=\sigma^2<\infty$. Sejam, $\epsilon>0$ e $0<\delta<1$. Se, $n>\dfrac{\sigma^{2}\epsilon^2}{\delta}$, então,


$$P(\mid \overline{X}_{n}-\mu \mid  <\epsilon)\geq 1-\delta,$$

ou seja, $\overline{X}_n$ converge em probabilidade para $\mu$.

:::


## Teorema de convergência

Seja $X_1, X_2, \dots, X_n$ uma a.a. de $X\sim Ber(0.5)$. Observe que,
$$
X_{i}=\left\{
\begin{array}{cc}
0, & \text{fracasso}\\
1, & \text{sucesso}\\
\end{array}
\right.,
i=1,2, \dots, n.
$$

A proporção amostral é determinada por
$$\hat{p}_{n}=\overline{X}_{n}=\dfrac{\sum_{i=1}^{n}X_{i}}{n}=\dfrac{X_{1}+X_{2}+\dots+X_{n}}{n}$$
\small

Para $n=1 \quad \Rightarrow \quad  \hat{p}_{1}=\dfrac{X_{1}}{1}.$

Para $n=2 \quad \Rightarrow \quad  \hat{p}_{2}=\dfrac{X_{1}+X_{2}}{2}.$


$\vdots$

Para $n=n \quad \Rightarrow \quad  \hat{p}_{n}=\dfrac{X_{1}+X_{2}+\dots+X_{n}}{n}.$


## Teorema de convergência

<!-- %\centering -->

![](LGN.png){width="100%"}

<!-- \begin{center}  -->
<!-- \scalebox{0.65}{\includegraphics{LGN.png}} -->
<!-- \end{center} -->

## Teorema central do limite

> O Teorema Central do Limite é um dos mais importantes resultados em toda área de Probabilidade e Estatística. Ele nos diz aproximadamente como a **média amostral** é distribuída.

:::{#thm-thm5}
## Teorema Central do Limite - TCL

Seja $X_1, X_2, \dots, X_n$ uma sequência de v.a.'s independentes com $E(X_i)=\mu_{i}$ e
$Var(X_i)=\sigma^2)<\infty$, $i=1, 2,\dots, n$. Tome $S_n = X_1 + X_2 + \dots + X_n$, então, sob determinadas condições gerais,

$$Z_{n}=\dfrac{S_{n}-E(S_{n})}{\sqrt{Var(S_{n})}}=\dfrac{S_{n}-\sum_{i=1}^{n}\mu_{i}}{\sqrt{\sum_{i=1}^{n}\sigma^{2}_{i}}} \rightarrow N(0,1).$$
A distribuição de $Z_{n}$ se aproxima da $N(0,1)$ quando $n\rightarrow \infty$.

:::


O @thm-thm5 nos diz que a ditribuição limite de $Z_{n}$ ($S_{n}$ padronizado) será a distribuição $N(0,1)$.


## Distribuição amostral da média

:::{#cor-cor2}
Seja $(X_1, X_2, \dots, X_n)$ uma a.a. de $X$ com $E(X) = \mu$ e $Var(X) = \sigma^2 <\infty$. Então, para $n\rightarrow \infty$,

$$Z_{n}=\dfrac{\overline{X}_{n}-\mu}{\sqrt{\sigma^2 /n}}\rightarrow N(0,1).$$

:::

* Em outras palavras $\overline{X}_{n}$ é assintoticamente distribuído como uma Normal com média $\mu$ e variância $\sigma^2/n$.

* Um aspecto importante sobre o @thm-thm5 é o fato de que nada é dito sobre a forma da **f.p ou f.d original**. Qualquer que seja a distribuição, dado que possui variância **finita**, a média amostral terá **aproximadamente** distribuição Normal para amostras grandes.

## Representação do TCL graficamente


<!-- \centering -->

![](TCL.png){width="90%"}

<!-- \begin{center}  -->
<!-- \scalebox{0.8}{\includegraphics{TCL.png}} -->
<!-- \end{center} -->

## Algumas distribuições exatas

* Se $X\sim Ber(\theta)$, então:
$$P(\bar{X}=\bar{x}_{n})={n\choose n\bar{x}_{n}}  \theta^{n\bar{x}_{n}}(1-\theta)^{n-n\bar{x}_{n}}, \bar{x}_{n}=0,1/n, 2/n, \dots, 1.$$

* Se $X\sim Pois(\theta)$, então:
$$P(\bar{X}=\bar{x}_{n})=\dfrac{e^{-n\theta} \theta^{n\bar{x}_{n}}}{n\bar{x}_{n}!}, \bar{x}_{n}=0,1/n, 2/n, \dots$$


* Se $X\sim Normal(\mu, \sigma^2)$, então:
$$\bar{X}_{n}\sim N(\mu, \sigma^2).$$


* Se $X\sim Exp(\theta)$, então:
$$\bar{X}_{n}\sim Gama(n, n\theta).$$


## Distribuição amostral da proporção

* Seja $X_1, X_2, \dots, X_n$ uma a.a. de $X \sim Ber(\theta)$.

* Em que $\theta$ representa a proporção de elementos com uma
determinada característica na população.

* Temos que

$$E(X_{i})=\theta \quad \text{e} \quad Var(X_{i})=\theta(1-\theta).$$

* Seja $S_{n}=X_{1}+X_{2}+\dots+X_{n}$, então a proporção amostral é definida por

$$\hat{p}=\dfrac{S_{n}}{n}=\dfrac{X_{1}+X_{2}+\dots+X_{n}}{n}=\bar{X}.$$

## Distribuição amostral da proporção

`Distribuição exata:`

* Temos que $S_{n}=X_{1}+X_{2}+\dots+X_{n}\sim Bin(n, \theta)$, então

$$P\left( \hat{p}=\dfrac{k}{n} \right) = {n \choose k} \theta^{k}(1-\theta)^{n-k}, k=0,1,2, \dots, n.$$

`Distribuição aproximada pelo TCL:`


* Temos que $\hat{p}=\dfrac{S_{n}}{n}=\dfrac{X_{1}+X_{2}+\dots+X_{n}}{n}=\bar{X}_{n}.$, então

$$\hat{p}\sim N\left( \theta, \dfrac{\theta(1-\theta)}{n}\right).$$




# [Distribução amostral de estatísticas de ordem]{style="float:right;text-align:right;"} {background-color="#027eb6"}


## Estatísticas de ordem amostrais


* Seja $X_1, X_2, \dots, X_n$ sendo uma sequência de variáveis
aleatórias i.i.d. com função de distribuição $F(\cdot)$.

* Podemos reordenar (de forma crescente) essa sequência da seguinte forma
$$X_{(1)}\leq X_{(2)}\leq \dots\leq X_{(n)}.$$

* No caso em que $F$ seja contínua, temos que
$$X_{(1)}< X_{(2)}< \dots< X_{(n)}.$$


* Uma vez que $P(X_i = X_j) = 0$ para todo $i \neq j$ , para variáveis
aleatórias contínuas.



## Estatísticas de ordem amostrais

* Para uma sequência de v.a's $X_1, X_2, \dots, X_n$ i.i.d., $X_{(k)}$ é denominada de $k-$ésima estatística de ordem.


* O mínimo é denotado por $X_{(1)}$:
$$X_{(1)}=\min(X_1, X_2, \dots, X_n).$$


* De maneira similar, o máximo é denotado por $X_{(n)}$:
$$X_{(n)}=\max(X_1, X_2, \dots, X_n).$$


## Estatísticas de ordem amostrais

Seja $g(x)$ e $G(X)$ as funções de densidade (probabilidade) e distribuição de $X$, respectivamente.


* Para o $X_{(1)}$:
   - A função de distribuição é dada por 

$$ F_{X_{(1)}}(x) = 1-(1-G(x))^n.$$

   - A função de densidade é dada por

$$ f_{X_{(1)}}(x) = \dfrac{d}{dx}F_{X_{(1)}}(x) = ng(x)(1-G(x))^{(n-1)}.$$



## Estatísticas de ordem amostrais


Seja $g(x)$ e $G(X)$ as funções de densidade (probabilidade) e distribuição de $X$, respectivamente.


* Para o $X_{(n)}$: 

   - A função de distribuição é dada por

$$ F_{X_{(n)}}(x) = G(x)^n.$$

   - A função de densidade é dada por

$$ f_{X_{(n)}}(x) = \dfrac{d}{dx}F_{X_{(n)}}(x) = ng(x)G(x)^{(n-1)}.$$


# [Amostrando da Normal - Média amostral]{style="float:right;text-align:right;"} {background-color="#027eb6"}



## Média amostral

* Esta seção lida com propriedades das quantidades amostrais provenientes de uma população Normal.

* A distribuição Normal tem papel importante nos estudos estatísticos.

* Muitas populações seguem a distribuição Normal com um bom grau de aproximação.

* Modelos estatísticos utilizando a distribuição Normal são amplamente considerados na literatura científica.

* Amostrar de uma população Normal leva a muitas propriedades úteis e também a muitas distribuições amostrais conhecidas.

` Média amostral e a população Normal:`

* A média amostral é uma das mais simples funções de uma amostra aleatória.

* Para uma amostra aleatória da distribuição Normal, a distribuição exata da média amostral também é Normal (para qualquer tamanho amostral n, ou seja, não precisamos do TCL para dar suporte a esta afirmação).



## Média amostral

:::{#thm-thm6} 
Seja $\overline{X}_{n}=\dfrac{1}{n}\sum\limits_{i=1}^{n}\left(X_{i}\right)$ a média amostral de uma amostra aleatória $X_1, \dots, X_n$ obtida da distribuição Normal com média $\mu$ e variância $\sigma^2$. Então, $\overline{X}_{n}\sim N\left(\mu, \sigma^2/n\right)$.
:::
**Prova:** Usaremos a função geradora de momentos (f.g.m),

$$
\begin{array}{ccccc}
m_{\overline{X}_{n}}(t)&=&E\left(e^{t\overline{X}_{n}}\right)&=&E\left[\exp\left\{\dfrac{t}{n}\sum\limits_{i=1}^{n}X_{i}\right\}\right] \\
&=& E\left[\prod\limits_{i=1}^{n}\exp\left\{\dfrac{t}{n}X_{i}\right\}\right] &\underbrace{=}_{X_{i}^{'}s\ ind.}& \prod\limits_{i=1}^{n}E\left[\exp\left\{\dfrac{t}{n}X_{i}\right\}\right]\\
&=& \prod\limits_{i=1}^{n}m_{X_{i}}(t/n) &=& \prod\limits_{i=1}^{n}\exp\left\{\mu\dfrac{t}{n}+\dfrac{\sigma^2}{2}\dfrac{t^2}{n^2}\right\}\\
&=&\exp\left\{\mu t+\dfrac{\sigma^2/n}{2}t^2\right\}. &\Rightarrow& \text{f.g.m da} \  N\left(\mu, \sigma^2/n\right)
\end{array}
$$

## Média amostral

* Dado que temos a distribuição exata de $\overline{X}_{n}$ quando estimamos $\mu$ com $\overline{X}_{n}$, seremos capazes de calcular, por exemplo, a probabilidade exata de que nosso estimador $\overline{X}_{n}$ esteja dentro de uma distância fixada do parâmetro desconhecido $\mu$.

# [Amostrando da Normal - Variância amostral]{style="float:right;text-align:right;"} {background-color="#027eb6"}

## Variância amostral

* A distribuição Normal possui dois parâmetros desconhecidos $\mu$ e $\sigma^2$.

* Vimos anteriormente a distribuição amostral de $\overline{X}_{n}$ que estima $\mu$.

* Procuremos agora pela distribuição de 

$$S^2=\dfrac{1}{n-1}\sum\limits_{i=1}^{n}(X_{i}-\overline{X})^2,$$

que estima o parâmetro $\sigma^2$.

* A distribuição Qui-Quadrado desempenha um papel fundamental na determinação da distribuição de $S^2$.


## Variância amostral

:::{#def-def8}

## Distribuição Qui-Quadrado

Seja $X$ uma v.a. com f.d

$$f(x)=\dfrac{1}{\Gamma(k/2)}\left(\dfrac{1}{2}\right)^{k/2}x^{k/2 -1} \exp\left\{ -\dfrac{1}{2}x\right\}\times I_{(0,\infty)}(x),$$

então dizemos que $X$ tem distribuição Qui-Quadrado com $k$ graus de liberdade ($k$ é um número inteiro positivo).

:::

`Notação:` $\chi^{2}_{k}=$ Qui-Quadrado com $k$ graus de liberdade.


* A densidade da Qui-Quadrado é um caso particular da densidade $Gama(r,\lambda)$, onde $r=k/2$ e $\lambda=1/2$.

* Se $X\sim Ga(r,\lambda)$ temos $f(x)=\dfrac{\lambda^r}{\Gamma(r)}x^{r-1}e^{\lambda x}, I_{(0,\infty)}(x)$.

* Fazendo $r=k/2$ e $\lambda=1/2$, tem-se na densidade definida anteriormente.



## Variância amostral

Temos também que:

$$E(X)=\dfrac{r}{\lambda}=\dfrac{k/2}{1/2}=k \quad \text{e} \quad Var(X)=\dfrac{r}{\lambda^2}=\dfrac{k/2}{1/2^2}=2k.$$

A f.g.m é dada por

$$m_{X}(t)=\left(1-\dfrac{t}{\lambda}\right)^{-r}=\left(1-\dfrac{t}{1/2}\right)^{-k/2}=\left(\dfrac{1}{1-2t}\right)^{k/2}, \quad \text{para todo} \quad t<\lambda=1/2.$$

:::{#thm-thm7} 
Se $X_{1}, \dots, X_n$ são v.a. Normais independentes com médias $\mu_i$ e variâncias $\sigma^2_{i}$ (Note que não prescisar ser i.i.d). Então, 

$$U=\sum\limits_{i=1}^{k}\left(\dfrac{X_{i}-\mu_{i}}{\sigma_{i}}\right)^2\sim \chi_{k}^{2}.$$
:::


## Variância amostral 

`Prova:` Seja $Z_{i}=\dfrac{X_{i}-\mu_{i}}{\sigma_{i}}$, em que $Z_{i}\sim N(0,1)$. Assim,


$$
\begin{array}{lllll}
m_{U}(t)&=&E\left[ e^{  tU  }\right]&=&E\left[ e^{  t  \sum_{i=1}^{k} Z_{i}^2}\right]\\
&=& E\left[\prod\limits_{i=1}^{k} e^{t Z_{i}^2}\right] &\underbrace{=}_{Z_{i}'s ind.}&\prod\limits_{i=1}^{k}E\left[ e^{t Z_{i}^2}\right].\\
\end{array}
$$

## Variância amostral

Mas,

$$
\begin{array}{lllll}
E\left[ e^{t Z_{i}^2}\right]&=&\displaystyle\int\limits_{-\infty}^{\infty}e^{t Z^2}(2\pi)^{-\frac{1}{2}}e^{-\frac{1}{2} Z^{2}}dZ=\displaystyle\int\limits_{-\infty}^{\infty}(2\pi)^{-\frac{1}{2}}e^{-\frac{1}{2} (Z^{2}-2tZ^2)}dZ && \\
&=& \displaystyle\int\limits_{-\infty}^{\infty}(2\pi)^{-\frac{1}{2}}e^{-\frac{1}{2} (1-2t)Z^{2}}dZ&& \\
&=&\dfrac{1}{\sqrt{1-2t}} \displaystyle\int\limits_{-\infty}^{\infty}\sqrt{1-2t}(2\pi)^{-\frac{1}{2}}\exp\left\{-\frac{1}{2\left(\frac{1}{(1-2t)}\right)} Z^{2}\right\}dZ&& \\
&=&\dfrac{1}{\sqrt{1-2t}} \displaystyle\int\limits_{-\infty}^{\infty}\left[2\pi\left(\frac{1}{(1-2t)}\right)\right]^{-\frac{1}{2}}\exp\left\{-\frac{1}{2\left(\frac{1}{(1-2t)}\right)} Z^{2}\right\}dZ&&
\end{array}
$$



## Variância amostral

* A expressão de dentro da integral é a densidade da $N(0,V)$, onde $V=\frac{1}{(1-2t)}$.

* O resultado da integral é, portanto, igual a $1$.

* Conclusão:

$$
E\left[ e^{t Z_{i}^2}\right]= m_{Z_{i}^{2}}(t)= \left(\frac{1}{1-2t}\right)^{\frac{1}{2}}, \ \text{para} \ t<1/2.
$$

`Nota:` O resultado acima determina que $Z_{i}^2\sim  \chi_{1}^{2}$, ou seja, se $Z_{i}\sim N(0,1)$ temos $Z_{i}^2\sim  \chi_{1}^{2}$.

$$
m_{U}(t)=\prod\limits_{i=1}^{k}E\left[ e^{t Z_{i}^2}\right]=\prod\limits_{i=1}^{k}\left(\frac{1}{1-2t}\right)^{\frac{1}{2}}=\left(\frac{1}{1-2t}\right)^{\frac{k}{2}}
$$
Portanto, $U\sim \chi_{2}^{k}$ finalizando a prova.


## Variância amostral



:::{#cor-cor3}
Se $X_{1}, \dots, X_n$ são v.a. Normais independentes com médias $\mu$ e variâncias $\sigma^2$


$$U=\sum\limits_{i=1}^{k}\dfrac{(X_{i}-\mu)^2}{\sigma^2}\sim \chi_{k}^{2}.$$

:::

Em palavras, o @thm-thm7 diz que a soma de quadrados de variáveis aleatórias N(0,1) independentes possui distribuição Qui-Quadrado com grau de liberdade igual ao número de termos na soma.


## Variância amostral


:::{#thm-thm8} 

Se $Z_{1},Z_2 \dots, Z_n$ é uma amostra aleatória da distribuição $N(0,1)$, então:

i. $\overline{Z}\sim N(0,1/n)$;

ii. $\overline{Z}$ e $\sum\limits_{i=1}^{n}(Z_{i}-\overline{Z})^2$ são independentes;

iii. $\sum\limits_{i=1}^{n}(Z_{i}-\overline{Z})^2\sim \chi^{2}_{n-1}$.

:::

`Prova:` (i) é um caso especial do @thm-thm7. A prova da parte (ii) será incompleta (somente o caso $n = 2$). Se $n=2$ temos $\overline{Z}=(Z_{1}+Z_{2})/2$ e 


$$
\begin{array}{lll}
\sum\limits_{i=1}^{2}(Z_{i}-\overline{Z})^2&=& (Z_{1}-(Z_{1}+Z_{2})/2)^2+(Z_{2}-(Z_{1}+Z_{2})/2)^2\\
&=&\dfrac{(Z_{1}-Z_{2})^2}{4}+\dfrac{(Z_{2}-Z_{1})^2}{4}= \dfrac{(Z_{2}-Z_{1})^2}{2}
\end{array}
$$

## Variância amostral

* Então, $\overline{Z}$ é função de $Z_{1}+Z_{2}$ e $\sum\limits_{i=1}^{2}(Z_{i}-\overline{Z})^2$ é função de $Z_{2}-Z_{1}$.

 
* Para mostrar que $\overline{Z}$ e $\sum\limits_{i=1}^{2}(Z_{i}-\overline{Z})^2$ são independentes basta mostrar que $Z_{1}+Z_{2}$ e $Z_{2}-Z_{1}$ são independentes.

Veja que

$$
\begin{array}{lllll}
m_{Z_{1}+Z_{2}}(t_1)&=& E\left[ e^{t_1 (Z_{1}+Z_{2})}\right] &=& E\left[ e^{t_1 Z_{1}} e^{t_1 Z_{2}}\right]\\
&\underbrace{=}_{Z_{i}' ind.}& E\left[ e^{t_1 Z_{1}}\right]E\left[ e^{t_1 Z_{2}}\right] &=& \exp\left\{\dfrac{1}{2}t_{1}^2 \right\}\exp\left\{\dfrac{1}{2}t_{1}^2 \right\}\\
&=&\exp\left\{t_{1}^2 \right\}.&&
\end{array}
$$
Similarmente, $m_{Z_{2}-Z_{1}}(t_2)=\exp\left\{t_{2}^2 \right\}$. Se $Z_{1}\sim N(0,1)$, então $-Z_{1}\sim N(0,1)$.
 
 
## Variância amostral


Além disso, temos que a $m_{Z_{1}+Z_{2},Z_{2}-Z_{1}}(t_1, t_2)$

$$
\begin{array}{lllll}
&=& E\left[ e^{t_1 (Z_{1}+Z_{2})+t_2 (Z_{2}-Z_{1})}\right]=E\left[ e^{(t_1-t_2)Z_{1}}e^{(t_1+t_2)Z_{2}}\right] && \\
&\underbrace{=}_{ind.}& E\left[ e^{(t_1-t_2)Z_{1}}\right]E\left[e^{(t_1+t_2)Z_{2}}\right]=\exp\left\{\dfrac{1}{2}(t_1-t_2)^2\right\}\exp\left\{\dfrac{1}{2}(t_1+t_2)^2\right\} &&  \\
&=&\exp\left\{\dfrac{1}{2}(t_1-t_2)^2\right\}\exp\left\{\dfrac{1}{2}(t_1+t_2)^2\right\} &&\\
&=& \exp\left\{\dfrac{1}{2}\left[t_1^2-2t_1t_2 +t_2^2+ t_1^2+2t_1t_2 +t_2^2 \right]\right\}=\exp\left\{t_1^2\right\}\exp\left\{t_2^2\right\} && \\
&=&m_{Z_{1}+Z_{2}}(t_1) m_{Z_{2}-Z_{1}}(t_2).&&
\end{array}
$$

Visto que a f.g.m. conjunta pode ser fatorada no produto das f.g.m.'s $Z_{1}+Z_{2}$ e $Z_{2}-Z_{1}$ são independentes.


## Variância amostral


(iii) Para provar essa parte, iremos supor que $\overline{Z}$ e $\sum\limits_{i=1}^{n}(Z_{i}-\overline{Z})^2$ para um $n$ arbitrário.

Note que:


$$
\begin{array}{lll}
\sum\limits_{i=1}^{n}Z_{i}^2&=&\sum\limits_{i=1}^{n}(Z_{i}-\overline{Z}+\overline{Z})^2\\
&=&\sum\limits_{i=1}^{n}(Z_{i}-\overline{Z})^2+2\overline{X}\underbrace{\sum\limits_{i=1}^{n}(Z_{i}-\overline{Z})}_{n\overline{Z}-n\overline{Z}=0} + n \overline{X}^2\\
&=& \sum\limits_{i=1}^{n}(Z_{i}-\overline{Z})^2+ n \overline{Z}^2
\end{array}
$$

## Variância amostral

Usando o resultado da parte (ii) temos que que $\overline{Z}$ e $\sum\limits_{i=1}^{n}(Z_{i}-\overline{Z})^2$ são independentes. Então:

$$
\begin{array}{lllll}
m_{\sum_{i=1}^{n}Z_{i}^{2}}(t)&=&m_{\sum_{i=1}^{n}(Z_{i}-\overline{Z})^2+ n \overline{Z}^2}(t)&\underbrace{=}_{ind.}& m_{\sum_{i=1}^{n}(Z_{i}-\overline{Z})^2}(t) m_{n \overline{Z}^2}(t)
\end{array}
$$

e 


$$
m_{\sum_{i=1}^{n}(Z_{i}-\overline{Z})^2}(t)=\dfrac{m_{\sum_{i=1}^{n}Z_{i}^{2}}(t)}{m_{n \overline{Z}^2}(t)}=\dfrac{\overbrace{\left(\dfrac{1}{1-2t}\right)^\frac{n}{2}}^{\chi_{n}^{2}}}{\underbrace{\left(\dfrac{1}{1-2t}\right)^\frac{1}{2}}_{\chi_{1}^{2}}}=\left(\dfrac{1}{1-2t}\right)^\frac{n-1}{2}.
$$

O resultado da f.g.m. da distribuição $\chi_{n-1}^{2}$.


## Variância amostral


* O @thm-thm8 foi definido para uma amostra aleatória da distribuição $N(0,1)$, entretanto se desejamos fazer inferência sobre $\mu$ e $\sigma^2$ devemos considerar uma amostra da $N(\mu, \sigma^2)$.

* Se $X_{1},X_{2}, \dots, X_{n}$ uma a.a. da $N(\mu, \sigma^2)$. Neste caso definimos $Z_{i}(X_{i}-\mu)/\sigma$. (ver `Teorema 8`)

* Parte (i) do @thm-thm8 se torna:

1. $\overline{Z}=\dfrac{1}{n}\sum_{i=1}^{n}(X_{i}-\mu)/\sigma=\dfrac{\overline{X}-\mu}{\sigma}\sim N(0,1/n)$ 

## Variância amostral

* Parte (ii) do @thm-thm8 se torna:



2. $\overline{X}=\dfrac{\overline{X}-\mu}{\sigma}$ e $\sum_{i=1}^{n}(Z_{i}-\overline{Z})^2=\sum_{i=1}^{n}\left[\dfrac{X_{i}-\mu}{\sigma}-\dfrac{\overline{X}-\mu}{\sigma}\right]^2=\sum_{i=1}^{n}\dfrac{(X_{i}-\overline{X})^2}{\sigma^2}$


são independentes, implicando que $\overline{X}$ e $\sum_{i=1}^{n}(X_{i}-\overline{X})^2$ também são.


* Parte (iii) do @thm-thm8 se torna


3. $\sum_{i=1}^{n}(Z_{i}-\overline{Z})^2=\sum_{i=1}^{n}\dfrac{(X_{i}-\overline{X})^2}{\sigma^2}\sim \chi_{n-1}^{2}$

conforme mostrado em ii.


## Variância amostral

:::{#cor-cor4} 

Considere $X_{1}, X_{2}, \dots, X_{n}$ uma a.a. da $N(\mu, \sigma^2)$. Seja $S^{2}=\dfrac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X})^2$ a variâcia amostral. Então,

$$U=\dfrac{(n-1)}{\sigma^2}S^{2}\sim \chi_{n-1}^{2}.$$

:::

`Prova:` Considere a parte (iii) da extensão do `Teorema 8`.

Observação:Como $S^2$ é uma função linear de $U$ no corolário acima, a densidade de $S^2$ pode ser obtida a partir da densidade de $U$.

$U\sim \chi_{n-1}^{2}$ e $S^2$ é dada por uma função monótona de $U$.

$S^2=\dfrac{\sigma^2}{n-1}U$, $U=\dfrac{(n-1)}{\sigma^2}S^{2}$ e $\dfrac{dU}{dS^2}=\dfrac{n-1}{\sigma^2}$ 

## Variância amostral

A função de densidade de $S^2$ é dada por

$$
\begin{array}{lll}
f_{S^{2}}(s^{2})&=&f_{U}\left[ \dfrac{(n-1)}{\sigma^2}s^{2} \right]\times \left| \dfrac{n-1}{\sigma^2} \right|, \quad \text{para} \quad s^2>0 \\
&=&\dfrac{1}{\Gamma\left( \frac{n-1}{2} \right)}\left(\dfrac{1}{2}\right)^{\frac{n-1}{2}}\left[\dfrac{(n-1)}{\sigma^2}s^{2} \right]^{\frac{n-1}{2}-1}\exp\left\{ -\dfrac{1}{2} \dfrac{(n-1)}{\sigma^2}s^{2} \right\}\dfrac{(n-1)}{\sigma^2} I_{(0,\infty)}(s^2)\\
&=& \dfrac{1}{\Gamma\left( \frac{n-1}{2} \right)}\left(\dfrac{1}{2}\right)^{\frac{n-1}{2}}\left[\dfrac{(n-1)}{\sigma^2} \right]^{\frac{n-1}{2}}\left[\dfrac{(n-1)}{\sigma^2} \right]^{-1}(s^2)^{\frac{n-1}{2}-1}\times\\
&\times &\dfrac{(n-1)}{\sigma^2} \exp\left\{ - \dfrac{(n-1)}{2\sigma^2}s^{2} \right\}\times I_{(0,\infty)}(s^2)\\
&=& \dfrac{1}{\Gamma\left( \frac{n-1}{2} \right)} \left[\dfrac{(n-1)}{2\sigma^2} \right]^{\frac{n-1}{2}}(s^2)^{\frac{n-3}{2}}\exp\left\{ - \dfrac{(n-1)}{2\sigma^2}s^{2} \right\}\times I_{(0,\infty)}(s^2)
\end{array}
$$

## Variância amostral 


* Todos os resultados desta subseção são desenvolvidos para o caso de populações Normais. 

* Pode ser provado que para nenhuma outra distribuição:

1. A média e a variância amostral são independentemente distribuídas.

2. A média amostral possui exatamente a distribuição Normal


# [Distribuição F]{style="float:right;text-align:right;"} {background-color="#027eb6"}

## Distribuição F

* Apresenta considerável interesse prático.

* É a distribuição da razão de duas variáveis aleatórias Qui-Quadrado independentes e divididas pelos seus respectivos graus de liberdade.

* Suponha que $U \sim \chi^2_m$ e $V \sim \chi^2_n$ são variáveis aleatórias independentes. A densidade conjunta será dada por:


$$f_{U,V}(u,v) = f_U(u) f_V(v)$$



$$\begin{array}{ll}
=& \frac{1}{\Gamma\left(\frac{m}{2}\right)} \left(\frac{1}{2}\right)^{\frac{m}{2}} u^{\frac{m}{2} - 1} \exp\left(-\frac{1}{2}u\right) I_{(0,\infty)}(u)
\\
&\times \frac{1}{\Gamma\left(\frac{n}{2}\right)} \left(\frac{1}{2}\right)^{\frac{n}{2}} v^{\frac{n}{2} - 1} \exp\left(-\frac{1}{2}v\right) I_{(0,\infty)}(v)\\
=&  \frac{1}{\Gamma\left(\frac{m}{2}\right) \Gamma\left(\frac{n}{2}\right) 2^{\frac{m+n}{2}}} u^{\frac{m}{2} - 1} v^{\frac{n}{2} - 1} \exp\left(-\frac{1}{2}(u+v)\right) I_{(0,\infty)}(u) I_{(0,\infty)}(v)
\end{array}
$$




## Distribuição F

* Desejamos encontrar a distribuição da quantidade 

$$X = \dfrac{\frac{U}{m}}{\dfrac{V}{n}}$$

conhecida como `razão de variâncias`.

:::columns
::::column
* Para encontrar a distribuição de $X$, considere:


$$X = \frac{\frac{U}{m}}{\frac{V}{n}} = \frac{n}{m} \frac{U}{V} \quad \text{e} \quad Y = V$$

$$
U = \frac{m}{n} Y X
$$
::::
:::: column
* Jacobiano da Transformação:

$$
\begin{vmatrix}
\frac{\partial U}{\partial X} & \frac{\partial U}{\partial Y} \\
\frac{\partial V}{\partial X} & \frac{\partial V}{\partial Y}
\end{vmatrix}
= 
\begin{vmatrix}
\frac{mY}{n} & \frac{mX}{n} \\
0 & 1
\end{vmatrix}
= \frac{mY}{n}
$$
::::
:::


## Distribuição F



* A distribuição conjunta de $X$ e $Y$ é dada por:

$$
\begin{array}{lll}
f_{X,Y}(x,y)& = &f_{U,V}\left(\frac{m}{n} yx, y\right) \cdot \left|\frac{m y}{n}\right| \cdot I_{(0,\infty)}(x) \cdot I_{(0,\infty)}(y)\\
&=& \frac{1}{\Gamma\left(\frac{m}{2}\right) \Gamma\left(\frac{n}{2}\right) 2^{\frac{m+n}{2}}}
 \left(\frac{m}{n} yx\right)^{\frac{m}{2} - 1}  y^{\frac{n}{2} - 1} \\
&\times& \exp\left\{-\frac{1}{2} \left[\frac{m}{n} yx + y\right]\right\} 
 \frac{m y}{n}  I_{(0,\infty)}(x)  I_{(0,\infty)}(y)
\end{array}
$$

## Distribuição F


* A distribuição marginal de $X$ é

$$\begin{array}{ll}
=& \int\limits_0^\infty f_{X,Y}(x, y) \, dy = \frac{1}{\Gamma\left(\frac{m}{2}\right) \Gamma\left(\frac{n}{2}\right) 2^{\frac{m+n}{2}}} \left(\frac{m}{n}\right)^{\frac{m}{2}} x^{\frac{m-2}{2}}\\
\times& \int\limits_0^\infty y^{\frac{m+n-2}{2}} \exp\left\{-\frac{1}{2} y \left(\frac{m}{n}x + 1\right)\right\} \, dy\\
=& \frac{1}{\Gamma\left(\frac{m}{2}\right) \Gamma\left(\frac{n}{2}\right)} 2^{-\frac{m+n}{2}} \left(\frac{m}{n}\right)^{\frac{m}{2}} x^{\frac{m-2}{2}} \dfrac{\Gamma\left(\frac{m+n}{2}\right)}{\left[\frac{1}{2} \left(\frac{m}{n}x + 1\right)\right]^{\frac{m+n}{2}}}\\
\times& \int\limits_0^\infty \dfrac{\left[\frac{1}{2} \left(\frac{m}{n}x + 1\right)\right]^{\frac{m+n}{2}}}{\Gamma\left(\frac{m+n}{2}\right)} y^{\frac{m+n}{2} - 1} \exp\left\{-y \frac{1}{2} \left(\frac{m}{n}x + 1\right)\right\} \, dy
\end{array}
$$


## Distribuição F

* Dentro da integral temos a densidade da 

$$Ga\left[\frac{m+n}{2}, \frac{1}{2} \left(\frac{m}{n} x + 1\right)\right]$$

* Portanto, esta integral resulta em 1. Logo,

$$f_X(x) = 
\frac{\Gamma\left(\frac{m+n}{2}\right)}{\Gamma\left(\frac{m}{2}\right) \Gamma\left(\frac{n}{2}\right)} 
\left(\frac{m}{n}\right)^{\frac{m}{2}} 
\dfrac{x^{\frac{m-2}{2}}}{ 
\left(1 + \frac{m}{n}x\right)^{\frac{m+n}{2}}} 
I_{(0, \infty)}(x)
$$


## Distribuição F

:::{#def-def9} 
## Distribuição F
 
Se $X$ é uma variável aleatória com a f.d $f_x (x)$ do slide anterior, então $X$ é
definida como uma variável aleatória apresentando distribuição $F$ com $m$ e $n$ graus de liberdade.
:::

* `Notação:` $X \sim F_{m,n}$
 

::: {.callout-note}
## Importante:

* A ordem na qual os graus de liberdade são indicados é importante visto que a f.d da  distribuição não é simétrica em relação a $m$ e $n$.

* Se a variável aleatória $F$ é obtida a partir da razão de duas variáveis aleatórias Qui-Quadrado independentes, divididas pelos seus respectivos graus de liberdade, então o grau de liberdade da variável no numerador deve ser indicado primeiro na notação da $F$.

:::


## Distribuição F


:::{#thm-thm9} 

Seja $U$ uma variável aleatória $\chi^2_m$ e $V$ uma variável aleatória $\chi^2_n$. Assuma também que $U$ e $V$ são independentes. Então: 

$$X = \frac{U / m}{V / n} \quad \text{tem distribuição} \quad F_{m, n}.$$
:::


:::{#cor-cor5} 

Se $ X_1, \dots, X_{m+1} $ é uma amostra aleatória de tamanho $ m+1 $ de uma população $ N(\mu_X, \sigma^2) $. Considere $ Y_1, \dots, Y_{n+1} $ uma amostra aleatória de tamanho $ n+1 $ de uma população $ N(\mu_Y, \sigma^2) $. 

:::

* Assuma que as duas amostras sejam independentes, então:

$$ \frac{1}{\sigma^2} \sum_{j=1}^{m+1} (X_j - \bar{X})^2 \sim \chi^2_m \quad \text{e} \quad \frac{1}{\sigma^2} \sum_{i=1}^{n+1} (Y_i - \bar{Y})^2 \sim \chi^2_n $$

e a estatística
$$ \frac{\sum_{j=1}^{m+1} (X_j - \bar{X})^2 / m}{\sum_{i=1}^{n+1} (Y_i - \bar{Y})^2 / n} \sim F_{m, n}.$$



# [Distribuição t-Student]{style="float:right;text-align:right;"} {background-color="#027eb6"}

## Distribuição t-Student

* Outra distribuição de considerável importância prática.

* Seja $Z\sim N(0,1)$ e $U\sim \chi_{k}^{2}$ v.a. independentes.

* Deseja-se encontrar a distribuição de

$$X=\dfrac{Z}{\sqrt{U/k}}.$$


* A densidade conjunta de $Z$ e $U$ é dada por


$$
\begin{array}{lll}
f_{Z,U}(z,u)&=&f_{Z}(z)f_{U}(u)\\
&=& (2\pi)^{-1/2}\exp\left\{ -\dfrac{1}{2}z^{2} \right\}\dfrac{1}{\Gamma{(k/2)}}\left(\dfrac{1}{2}\right)^{k/2}u^{\frac{k}{2}-1}\exp\left\{ -\dfrac{1}{2}u \right\}I_{(-\infty,\infty)}(v)I_{(0,\infty)}(u)
\end{array}
$$

## Distribuição t-Student

* Considere a seguinte transformação:


$X=\dfrac{Z}{\sqrt{U/k}}$, $Y=U$ e $Z=X\sqrt{Y/k}$, então

$$\left|
\begin{array}{cc}
\frac{\partial Z}{\partial X}= \sqrt{\frac{Y}{k}} & \frac{\partial U}{\partial X}= 0\\
\frac{\partial Z}{\partial X}= \frac{X}{2k}\left(\frac{Y}{k}\right)^{-1/2} & \frac{\partial U}{\partial Y}= 1
\end{array}
\right|=\sqrt{\frac{Y}{k}}
$$

* Logo,

$$
\begin{array}{lll}
f_{XY}(x,y)&=&f_{Z,U}(x\sqrt{y/k},y)\times\sqrt{\frac{y}{k}} \\
&=& (2\pi)^{-1/2}\exp\left\{ -\dfrac{1}{2}\frac{x^2 y}{k} \right\}\dfrac{1}{\Gamma{(k/2)}}\left(\dfrac{1}{2}\right)^{k/2}y^{\frac{k}{2}-1}\exp\left\{ -\dfrac{1}{2}y \right\}\left(\frac{y}{k}\right)^{1/2} I_{(-\infty,\infty)}(x)I_{(0,\infty)}(y)
\end{array}
$$


## Distribuição t-Student


$$
\begin{array}{lll}
f_{X}(x)&=&\int\limits_{0}^{\infty}f_{XY}(x,y)dy \\
&=& (2k\pi)^{-1/2}\dfrac{1}{\Gamma{(k/2)}}\left(\dfrac{1}{2}\right)^{k/2}\displaystyle\int\limits_{0}^{\infty}y^{\frac{k}{2}+\frac{1}{2}-1}\exp\left\{ -\dfrac{1}{2}y\left[\frac{x^2}{k}+1\right] \right\}dy
\end{array}
$$

* Dentro da integral temos o núcleo da: $\quad Ga\left[ \dfrac{k+1}{2}; \dfrac{1}{2}\left(\frac{x^2}{k}+1\right) \right]$

## Distribuição t-Student


$$
\begin{array}{lll}
f_{X}(x)&=& (2k\pi)^{-1/2}\dfrac{1}{\Gamma{(k/2)}}\left(\dfrac{1}{2}\right)^{k/2}\dfrac{\Gamma{\left(\dfrac{k+1}{2}\right)}}{\left[\dfrac{1}{2}\left(\frac{x^2}{k}+1\right)\right]^{\dfrac{k+1}{2}}}\times\\
&\times& \underbrace{\displaystyle\int\limits_{0}^{\infty}\dfrac{\left[\dfrac{1}{2}\left(\frac{x^2}{k}+1\right)\right]^{\dfrac{k+1}{2}}}{\Gamma{\left(\dfrac{k+1}{2}\right)}}y^{\frac{k+1}{2}-1}\exp\left\{ -\dfrac{1}{2}y\left[\frac{x^2}{k}+1\right] \right\}dy}_{1}
\end{array}
$$


## Distribuição t-Student


* Assim,

$$
\begin{array}{lll}
f_{X}(x)&=& (2k\pi)^{-1/2}\dfrac{1}{\Gamma{(k/2)}}\left(\dfrac{1}{2}\right)^{k/2}\dfrac{\Gamma{\left(\dfrac{k+1}{2}\right)}}{\left[\dfrac{1}{2}\left(\frac{x^2}{k}+1\right)\right]^{\dfrac{k+1}{2}}}\\
&=& \dfrac{\Gamma{\left(\dfrac{k+1}{2}\right)}}{\Gamma{\left(\dfrac{k}{2}\right)}}\dfrac{1}{\sqrt{k\pi}}\dfrac{1}{\left(\frac{x^2}{k}+1\right)^{\dfrac{k+1}{2}}}I_{(-\infty,\infty)}(x).
\end{array}
$$


## Distribuição t-Student


`Teorema 9 (Distribuição t-Student):`  Se $X$ é uma v.a. com f.d. igual a anterior, então dizemos que $X$ segue a distribuição t-Student com $k$-graus de liberdade.

* Notação:$t_{k}$.

`Teorema 10:`  Se $Z\sim N(0,1)$ e $U\sim\chi_{k}^{2}$ são v.a's independentes, então 

$$X=\dfrac{Z}{\sqrt{U/k}}\sim t_{k}.$$


## Distribuição t-Student


O seguinte corolário mostra como o resultado do **Teorema 10** é
aplicável quando consideramos a amostragem a partir da distribuição
Normal.


`Corolário 4:` Considere $X_{1}, X_{2}, \dots, X_{n}$ uma a.a. da $N(\mu, \sigma^2)$, então $Z=(\overline{X}-\mu)/(\sigma/\sqrt{n})\sim N(0,1)$  e $\dfrac{1}{\sigma^2}\sum_{i=1}^{n}(X_{i}-\overline{X})^2\sim \chi_{n-1}^{2}$.

* Além disso $Z$ e $U$ são independentes (`Teorema 8`). Sendo assim, a seguinte v.a

$$\dfrac{Z}{\sqrt{U/n-1}}=\dfrac{(\overline{X}-\mu)/(\sigma/\sqrt{n})}{\sqrt{\dfrac{1}{\sigma^2}\dfrac{\sum_{i=1}^{n}(X_{i}-\overline{X})^2}{n-1}}}=\dfrac{\sqrt{n(n-1)}(\overline{X}-\mu)}{\sqrt{\sum_{i=1}^{n}(X_{i}-\overline{X})^2}}=\dfrac{(\overline{X}-\mu)}{\sqrt{S^2/n}}\sim t_{n-1}.$$


## Distribuição t-Student


`Observações:`

* Quando o grau de liberdade é igual a 1, a distribuição t-Student se torna a distribuição Cauchy.

* Conforme o grau de liberdade cresce, a distribuição t-Student se 
aproxima da distribuição $N(0,1)$.

* O quadrado de uma variável aleatória com distribuição $t_k$ terá 
distribuição $F_{1,k}$ . Isto é, se $Y\sim t_k$ então $Y^{2}\sim F_{1, k}$ .

* Se $X$ é uma variável aleatória com distribuição $t_{k}$, então


$E(X)=0,\quad \text{se} \ k>1$ e $Var(X)=\dfrac{k}{k-2},\quad \text{se} \ k>2$.








